{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuH4OZRsqhOHecJtzCwxJi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaInthiyaz/OasisInfobite_Data-analytics/blob/main/Autocomplete_and_Autocorrect_Data_Analytics_()P9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Idea:  **Autocomplete and Autocorrect Data Analytics **\n",
        "\n",
        "\n",
        "\n",
        "**Description:**\n",
        "\n",
        "\n",
        "\n",
        "Explore the efficiency and accuracy of autocomplete and autocorrect algorithms in natural\n",
        "language processing (NLP) through this data analytics project. The objective is to enhance user\n",
        "experience and text prediction by analyzing large datasets and implementing or optimizing\n",
        "autocomplete and autocorrect functionalities.\n",
        "\n",
        "\n",
        "**Key Concepts and Challenges:**\n",
        "\n",
        "Dataset Collection: Gather diverse text data.\n",
        "NLP Preprocessing: Clean and prepare data for analysis.\n",
        "Autocomplete: Implement algorithms for word/phrase predictions.\n",
        "Autocorrect: Optimize algorithms for spelling error correction.\n",
        "Metrics: Define and measure performance metrics.\n",
        "User Experience: Assess impact through feedback and surveys.\n",
        "Algorithm Comparison: Evaluate different models for efficiency and accuracy.\n",
        "Visualization: Use tools for data visualization.\n"
      ],
      "metadata": {
        "id": "BJm0clOn2mpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd  # Data manipulation\n",
        "import numpy as np  # Numerical operations\n",
        "import re  # Regular expressions for text cleaning\n",
        "from collections import Counter  # Counting word frequency\n",
        "from textblob import TextBlob  # Simple autocorrect and NLP tools\n",
        "import matplotlib.pyplot as plt  # Visualization\n",
        "import seaborn as sns  # Enhanced plots\n",
        "\n",
        "# ----------------------\n",
        "# Step 1: Dataset Collection\n",
        "# ----------------------\n",
        "# Load the credit card dataset for analysis.\n",
        "data = pd.read_csv(\"creditcard.csv\")  # Load dataset\n",
        "print(\"Dataset Shape:\", data.shape)  # Print number of rows and columns\n",
        "\n",
        "# ----------------------\n",
        "# Step 2: NLP Preprocessing (if applicable)\n",
        "# ----------------------\n",
        "# Check if the dataset contains a text field for autocomplete/autocorrect analysis.\n",
        "if 'Description' in data.columns:\n",
        "\n",
        "    def clean_text(text):\n",
        "        \"\"\"Lowercase and remove non-alphabetical characters.\"\"\"\n",
        "        text = str(text).lower()  # Convert to lowercase\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove punctuation and digits\n",
        "        return text\n",
        "\n",
        "    # Apply text cleaning to the 'Description' column.\n",
        "    data['clean_description'] = data['Description'].apply(clean_text)\n",
        "\n",
        "    # Tokenize cleaned text into words.\n",
        "    tokens = []\n",
        "    for sentence in data['clean_description'].dropna():\n",
        "        tokens.extend(sentence.split())\n",
        "\n",
        "    # ----------------------\n",
        "    # Step 3: Word Frequency for Autocomplete\n",
        "    # ----------------------\n",
        "    word_counts = Counter(tokens)  # Count word frequency\n",
        "\n",
        "    def autocomplete(prefix):\n",
        "        \"\"\"Suggests the most frequent words starting with the given prefix.\"\"\"\n",
        "        suggestions = [word for word in word_counts if word.startswith(prefix)]\n",
        "        return sorted(suggestions, key=lambda x: word_counts[x], reverse=True)[:5]\n",
        "\n",
        "    print(\"Autocomplete suggestions for 'fra':\", autocomplete(\"fra\"))  # Example\n",
        "\n",
        "    # ----------------------\n",
        "    # Step 4: Autocorrect with TextBlob\n",
        "    # ----------------------\n",
        "    def autocorrect(word):\n",
        "        \"\"\"Returns the most probable correction for a given word.\"\"\"\n",
        "        blob = TextBlob(word)\n",
        "        return str(blob.correct())\n",
        "\n",
        "    print(\"Autocorrect suggestion for 'fraudlent':\", autocorrect(\"fraudlent\"))\n",
        "\n",
        "    # ----------------------\n",
        "    # Step 5: Metrics and Visualization\n",
        "    # ----------------------\n",
        "    # Visualize the top 10 most common words.\n",
        "    most_common_words = word_counts.most_common(10)\n",
        "    words, counts = zip(*most_common_words)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=list(counts), y=list(words), palette=\"viridis\")\n",
        "    plt.title(\"Top 10 Most Common Words in Transaction Descriptions\")\n",
        "    plt.xlabel(\"Count\")\n",
        "    plt.ylabel(\"Words\")\n",
        "    plt.show()\n",
        "\n",
        "    # ----------------------\n",
        "    # Step 6: User Experience Simulation\n",
        "    # ----------------------\n",
        "    user_feedback = np.random.randint(7, 10, size=10)  # Simulate user ratings\n",
        "    print(\"User Satisfaction Scores:\", user_feedback)\n",
        "    print(\"Average User Score:\", np.mean(user_feedback))\n",
        "\n",
        "    # ----------------------\n",
        "    # Step 7: Algorithm Comparison Notes\n",
        "    # ----------------------\n",
        "    print(\"Consider testing SymSpell, Levenshtein Distance, or transformer-based models for advanced autocomplete/autocorrect.\")\n",
        "\n",
        "else:\n",
        "    print(\"No suitable text-based column (e.g., 'Description') found in the dataset for autocomplete/autocorrect analysis.\")\n"
      ],
      "metadata": {
        "id": "uej5h338AIU1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c320696-67c1-4408-bea2-db6bfc98f67f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Shape: (284807, 31)\n",
            "No suitable text-based column (e.g., 'Description') found in the dataset for autocomplete/autocorrect analysis.\n"
          ]
        }
      ]
    }
  ]
}